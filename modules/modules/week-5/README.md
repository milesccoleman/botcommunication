# Week 5: Chatbots

## Machines as Guides

Chatbots are used as resources to answer frequently asked questions, give banking information, help to offer healthcare feedback, and the list goes on. Increasingly they help save valuable time and resources by reducing the need for customers to ask the "call center," or to traverse pages and pages of technical documentation, or even to give "pitches" for ideas or products. 

Private, public, and nonprofit sectors can better support the people they serve by including topic-specific assistants, designed to guide them to the information they need and/or to support them in accomplishing particular tasks. 

In the previous module, we discussed the differences between monologic and dialogic scripts. Chatbots--as implied by the word "chat"--rely on dialogic scripts, and are designed to respond dynamically to the inquiries of users. On our way to designing conversational interfaces in the module directly following this one, we will start first with some consideration of an important aspect of our design of conversational interfaces: persona. 

But, first we need to make a quick stop at the often too-simplistic assumption that good chatbots are human-like. 



### It's Not as Simple as "They Remind Us of Humans" 

in 1966, Joeseph Wiezenbaum--a now famed and accomplished computer scientist--shared a sofware program with the world. ELIZA, the name of the program, and it was designed to engage people in ways that recreated a "back and forth" of conversation between a human and machine. Weizenbaum, ten years later, in his book _Computer Power and Human Reason_, describes a specific script he wrote--DOCTOR--which was a machinic parody of a Rogerian psychotherapist, meant to reveal how trivial, superficial, and artificial conversatons between people and machines actually are. Much to his surprise, he found not only that people positively engaged the bot, but that they even opened up to the bot much like they were interacting with a real therapist. 

![](../../../.gitbook/assets/eliza_conversation.jpg)

Weizenbaum's explanation for this is that people were ascribing human characteristics to the bot, and therefore could confide in the software program as if they were engaging a real person. 

Weizenbaum's anthropomorphism conclusion is complicated, however, by work like Nass and Moon's study of "Machines and Mindlessness: Social Responses to Computers." _Mindlessness_ as Nass and Moon desribe it, helps to account for the ways in which people  apply social scripts to machines, even when they realize that they are _not social entities_. 

![](../../../.gitbook/assets/dec_vt100_terminal.jpg)

When people expect "please" and "thank you" from a machine, and will even return the sentiments in conversation with a machine, it is not based on concious acknolwedgement of the entity they are communicating with; it is because they are on "auto-pilot," communicating in ways that represent the routines and rituals of everyday communicaiton \(which is mostly with other humans\).

So, technically, yes, we want bots that demonstrate an understanding of human social scripts. But that does not mean that we like some bots simply because they remind us of humans. Perhaps a counterargument to Weizenbaum would be that people confided in ELIZA exactly because they knew it was a _machine_. And, as such, ELIZA would withold judgement \(a human\) allowing the person to speak freely. The human-script based nicety of saying, "I am sorry to hear that you are depressed," does not lead the person to know that they are conversing with a human--it just allows more mindless, free flowing communication with a machine. 

Further reason for looking to fill specific communication niches and needs with machines can be found in Georgia State's POUNCE chatbot, designed to incoming students register for classes, sign up for financial aid, and access campus resources. 

{% embed url="https://blog.admithub.com/case-study-how-admithub-is-freezing-summer-melt-at-georgia-state-university" %}

The important finding from the case study of the POUNCE chatbot was that first-generation, and Pell Grant recieving students \(students coming from lower socio-economic statuses\) had higher rates of engagement regarding questions about registration, financial aid, et cetera than years prior without the chatbot. What this means is that students who might be afraid to ask "dumb" questions, because they are unfamiliar with the terrain of higher education, felt freer to ask the chatbot. Agian, was this because they thought it was human? \(Probably not.\) 



### Chatbot Personas





